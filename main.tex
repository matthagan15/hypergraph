\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amsthm, amssymb}
\usepackage{mathrsfs}
% \usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{xfrac}
\usepackage{algorithm,algpseudocode}
\usepackage{todonotes}
\usepackage{qcircuit}
\usepackage{smartref}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{topaths,calc}



%%%%%%%%    NOTATION DEFINITIONS FOR EASIER WRITING
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\braket}[2]{\langle #1|#2\rangle}
\newcommand{\ketbra}[2]{| #1\rangle\! \langle #2|}
\newcommand{\parens}[1]{\left( #1 \right)}
\newcommand{\brackets}[1]{\left[ #1 \right]}
\newcommand{\norm}[1]{\left| \left| #1 \right| \right|}
\newcommand{\diamondnorm}[1]{\left| \left| #1 \right| \right|_\diamond}
\newcommand{\anglebrackets}[1]{\left< #1 \right>}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\openone}{\mathds{1}}
\newcommand{\expect}[1]{\mathbb{E}\brackets{#1}}
\newcommand{\prob}[1]{\text{Pr}\left[ #1 \right]}
\newcommand{\textprob}[1]{\text{Pr}\left[ \text{#1} \right]}
\newcommand{\bigo}[1]{\mathcal{O}\left( #1 \right)}
\newcommand{\bigotilde}[1]{\widetilde{\mathcal{O}} \left( #1 \right)}
\newcommand{\ts}{\textsuperscript}
\newcommand{\field}{\mathbb{F}}


%%%%%%%%    VARIABLE DEFINITIONS FOR EASY CHANGING LATER
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}

\newcommand{\ChanGUE}{\mathcal{T}_{GUE}}
\newcommand{\ChanHaar}{\mathcal{T}_{Haar}}

%%%%%%%%    OPERATOR DEFINITIONS FOR EASIER MATHS
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\card}{card}
\newcommand{\cardi}[1]{\card \parens{ #1 }}
\newcommand{\trace}[1]{\Tr \brackets{ #1 }}
\newcommand{\partrace}[2]{\Tr_{#1} \parens{ #2 }}

\newcommand{\complex}{\mathbb{C}}
\newcommand{\matn}{\mathbb{M}_n}
% \newcommand{\identity}{\mathbbm{1}}
\newcommand{\identity}{\mathds{1}}
\newcommand{\ident}{\mathbb{I}}
\newcommand{\hilbspace}{\mathscr{H}}
\newcommand{\partfun}{\mathcal{Z}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{defn}{Definition}
% \DeclareMathOperator*{\dim}{dim}

\newcommand\disjointUnion{\rotatebox[origin=c]{180}{$\prod$}}

\title{Properties of Hypergraphs}
\author{Matthew Hagan}
\date{\today}

\begin{document}



\maketitle
\begin{abstract}
	We propose new definitions that allow for the study of spectral properties of hypergraphs. We first study the vector spaces that directed, weighted hypergraphs act on as linear opertors. We then study the differences between the complete hypergraph as compared to a complete graph representation and show how this leads to a quantity we denote $\card(.)$.
    We give a simple algorithm to output an estimate of $\card(H)$ for a hypergraph $H$ that satisfies a PAC-style bound.
\end{abstract}
% \tableofcontents

\section{Introduction}
Before a spectral theory of hypergraphs can be defined, a decent notion of spectral graph theory is good to have in mind. A graph $G$ is typically viewed as two sets $G = (N, E)$, one set $N = \set{n_i}_{i=1}^n$ of nodes ($V$ is reserved for vector spaces later) and one set of edges $E = \set{(n_a, n_b, w_{a,b})}$, where $n_a$ represents the head node and $n_b$ the tail, if this edge is directed, and $w_{a,b} \in \field$ the weight. The field $\field$ is left arbitrary for now, all we require is that the edge weights be in a field so we can construct vector spaces later.

We primarily focus on directed, weighted hypergraphs $H$ in which we take a node set $N$ and construct a set of hyperedges, also denoted $E$ as in the case for graphs, where a single hyperedge is defined as , which is the same as for graphs except that the head and tail are now subsets of $N$ as opposed to single elements. We will use $\alpha(e)$ to denote the head/input subset of a hyperedge and $\beta(e)$ to denote the tail/output subset. 

The study of the eigenvalues of linear operators associated to graphs, such as walk operators $W$, Adjacency matrices $A$, or Laplacians $L$, is known as spectral graph theory. This vector space is straightforwardly constructed by assigning a single basis element to each element $n_i \in N$, which can be thought of as a free module $F(N)$. We use Dirac notation to indicate vectors $\ket{n_i}$ as opposed to arrows $\vec{n_i}$. 

maybe talk about applications of spectral graph theory?

Hypergraphs are a direct generalization of a graph. They are defined as a tuple $(N, E)$ of nodes $N$ and hyperedges $E$. We will use directed, weighted hyperedges defined as a tuple $e = (\alpha(e) \subseteq N, \beta_e \subseteq N, w_e)$, where $\alpha(e)$ represents the input subset, $\beta(e)$ the tail, and $w_e$ the edge weight. From this we can index the power set $2^N$ which gives us access to a good basis for the hypergraph vector space. 

To study linear operators related to hypergraphs we will need to construct a slightly more intricate vector space. To start, we note the following useful property of the power set 
\begin{equation}
    2^N =  \bigcup_{k=0}^{|N|} N_k = \bigoplus_{k=0}^{k = |N|} N_k,
\end{equation}
where $N_k := \set{X \subseteq N : |X| = k}$ is the set of subsets of $N$ of cardinality $k$ and we use $\bigoplus$ to denote the disjoint union, or coproduct, when working in the category Set. Given this set $2^N$ we can now construct a new vector space by simply assigning each element $s_i \in 2^N$ a basis vector $\ket{s_i}$. We use $s_i$ to denote a single indexed subset of $N$. 

\begin{defn}[Hyperspace]
    Let $F$ denote the free module action and $N = \set{n_i}_{i=1}^n$. We call the vector space $V = \bigoplus_{k=0}^n V_k = F(2^N) = F(\bigoplus_{k=0}^n N_k) = \bigoplus_{k=0}^n F(N_k)$ associated with a hypergraph the associated hyperspace. A hyperspace $V$ comes with projectors $\Pi_k : V \to V_k$ and inclusions $\eta_k : V_k \to V$. We denote the natural basis $\ket{s_i}$, where each $s_i \subset N$ is an indexed subset of $N$ the subset basis of the hyperspace. This basis allows for a function $\cardi{\ket{\set{n_1}}} = 1$ which gives the cardinality of the underlying set. We denote the useful combination $\widetilde{\Pi}_k = \eta \circ \Pi_k : V \to V$ which acts as
    \begin{equation}
        \widetilde{\Pi_k} \ket{s_i} = \begin{cases}
            0 & \text{if } s_i \notin N_k \\
            \ket{s_i} & \text{if } s_i \in N_k 
        \end{cases}
    \end{equation}
\end{defn}

We now can define the adjacency matrix for the hypergraph
\begin{equation}
    A_H = \sum_{i=1}^{2^n} \sum_{j=1}^{2^n} w_{i,j} \ketbra{s_i}{s_j},
\end{equation}
which is very standard (is that even useful?). We can study probabilistic processes or quantum systems modeled by hypergraphs by restricting our attention to isometric adjacency matrices, either $L_1, L_2, \ldots$. 
Similarly a laplacian $L_H = D - A$, where $D$ is the "degree" of each basis vector defined in the usual graph theoretic sense, is straightforward to define and is a positive semi-definite operator. 

Now that we have seen how hypergraphs can be represented as adjacency matrices on an exponentially larger space, we are led to the following question: If matrices can be represented as weighted directed graphs, what is the difference between a hypergraph on a set of size $n$ and a weighted, directed graph on a space of size $2^n$? To isolate the differences we will utilize the complete hypergraph on $N$ and a complete graph on $2^N$, without the knowledge that the set $2^N$ is a power set. 

Let $H_C$ denote the complete hypergraph and $G_C$ the respective complete graph. To isolate differences we will use the concept of cardinality of the basis vectors. Define the random variable $\card(.)$ as follows
\begin{align}
    \card(\ket{\set{}}) &= 0 \\
    \card(\ket{\set{n_1}}) &= 1 \\
    \expect{\cardi{\frac{1}{2} \ket{\set{n_1, n_2}} + \frac{1}{2} \ket{\set{}} }} &= 1 \\
    \prob{\cardi{\sum_{i} c_i \ket{s_i}} = k } & \coloneqq \frac{\sum_{i,j} c_i \bra{s_j} \Pi_k \ket{s_i}}{ \sum_i c_i}.
\end{align}
We now look at the complete graph and hypergraphs. Let $\ket{e} = \frac{1}{\sqrt{2^n}} \sum_{i=1}^{2^n} \ket{s_i}$ denote the $L_2$ normalized all ones vector in the subset basis. The walk operator, or adjacency operator, for the complete hypergraph is then simply $H_C = \ketbra{e}{e}$, or the all ones matrix properly normalized. As a hypergraph we have access to the $\card$ operator, which allows us to compute the cardinality distribution of the dominant eigenvector of $H_C$ as
\begin{align}
    \cardi{H_C(n), k} := \prob{\cardi{\ket{e}} = k } = \frac{\binom{n}{k}}{2^n}.
\end{align}
We can generalize this notion to an arbitrary stochastic hypergraph $H$  by replacing $\ket{e} \mapsto \lim_{m \to \infty} \int H^m \ket{\psi} ~d\ket{\psi} $, which is simply a method of obtaining the dominant eigenvectors.

\begin{theorem}[Stirling's]
    \begin{equation}
        n! = (1 + o(1)) \sqrt{2 \pi n} n^n e^{-n}
    \end{equation}
    \begin{proof}
    \begin{align}
        n! &= \int_0^\infty t^n e^{-t} dt \\
        &= \int_0^\infty (s+n)^{n} e^{-(s+n)} ds \\
        &= n^n e^{-n} \int_0^\infty \parens{ 1 + \frac{s}{n}}^{n} e^{-s)} ds \\
        &= n^n e^{-n + n \log n} \int_0^\infty e^{n \log \parens{1 + \frac{s}{n}} - s} ds \\
        &= n^n e^{-n + n \log n} \int_0^\infty e^{n \log \parens{1 + \frac{s}{n}} - s} ds \\
        &= \int_0^\infty t^n e^{-t} dt 
    \end{align}
\end{proof}
\end{theorem}

\begin{align}
    \binom{n}{k} &= \frac{n!}{(n-k)! k!} \\
    &= \frac{(1 + o(1))\sqrt{2 \pi n} n^n e^{-n}}{(1+o(1)) 2 \pi \sqrt{n (n-k)} k^k (n-k)^{n-k} e^{-n}} \\
    &= \frac{1 + o(1)}{\sqrt{2 \pi}(1+o(1))} \frac{n^n}{ k^k (n-k)^{n-k + 1/2}}
\end{align}

For the complete hypergraph we note that we can easily compute quantities such as the expected cardinality and the entropy
\begin{align}
    \expect{\cardi{H, k}} &= \frac{n}{2} \\
    \mathbf{H} \brackets{\cardi{H, k}} &= - \sum_{k=0}^{n} \frac{\binom{n}{k}}{2^n} \log \parens{\frac{\binom{n}{k}}{2^n}}\\
\end{align}


\begin{figure}
    \begin{tikzpicture}
        \node (n1) at (0,0) {};
        \node (n2) at (1,1) {};
        \node (n3) at (2,0) {};
        \node (n4) at (3, 3) {};
        \node (n5) at (4, 2) {};
        \foreach \v in {1,2,3,4,5} {
            \fill (n\v) circle (0.1);
        }
    \end{tikzpicture}
    \caption[Hypergraph]{An example of a directed, weighted hypergraph with a single edge $e$}


\end{figure}

\begin{defn}[Expected Cardinality]
    Given query access to hypergraph $H$ that is an $L_1$ isometry, output $1$ if the quantity $\expect{\cardi{H,k}} \geq \frac{n}{2}$.
\end{defn}


\section{Algebraic Construction}
Now that we have a vector space, we can define a hyperedge $e$ simply as a linear map on this overall vector space, or in other words as a $\field$-module endomorphism
\begin{equation}
    e \in \Hom_{\field} (V, V).
\end{equation}
This condition of linearity is per usual. We say $e$ is a $\field$-module homomorphism (aka linear map) if for all scalars $c \in \field$ and vectors $u, v \in V$
\begin{align}
    e(c * v) = c * e(v) \\
    e (u + v) = e(u) + e(v).
\end{align}
Note that we can add elements of $V$ due to the underlying abelian group structure of the module. 

We then take another step and ask the question, can we induce a module structure on the set of linear maps $\Hom_{\field}$? Since $\field$ is a field we are allowed to do so (we only really need commutativity of $\field$ ). First define the action of $c \in \field$ on $e \in \Hom_\field(V, V)$ in the obvious way $(c * e) (v) := c * e(v)$. This may seem like simply removing the parenthesis, but the question of linearity shows it is a different matter:
\begin{align}
    (c * e) (s * v) &= c * e( s * v) \\
    &= (c * s) * e(v) \\
    &\overset{!}{=} s * (c * e) (v),
\end{align}
where the ! means we used commutativity to get $(c *e) (s * v) = s * (c * e) (v)$. Addition follows straightforwardly, $(e+f) (v) \coloneqq e(v) + f(v)$. This all may seem redundant, but it's important to spell out that everything is legal.

So we now have hyperedges defined as a specific linear map $e$ acting on a vector space $V$, and that we can add hyperedges and multiply them by some field. We then can simply define a hypergraph $h = \sum c_i e_i$, a weighted sum of hyperedges. Thats it! 


\section{Preliminaries}
\begin{itemize}
    \item Modules
    \item Free Modules
    \item Coproducts
    \item 
\end{itemize}
\section{Construction of a unique Linear Map}
\subsection{Formal Definition}
\subsection{Informal Discussion}

\section{Applications}
\subsection{Two State Discrete Time Quantum Systems}
\subsection{epidemiology}
\subsection{MIMO}
\subsection{local-to-global}

\section{Discussion}
A linear theory of hypergraph connections allows for immediate generalizations of many previous concepts. We posit a few conjectured areas that could lead to fruitful future research.
\begin{itemize}
    \item There is a beautiful theorem by Sunada that states that a graph $G$ is an optimal expander, aka the graph is Ramanujan, if and only if it's Ihara zeta function satisfies a Graph variant of the Riemann Hypothesis. Our theory now allows for these questions to be generalized to the hypergraph setting.
    \item One can impose that a hypergraph maintains the global vector given a certain norm, for example an $L_1$ normalized theory would consitute a model that conserves probability. If one defines a parametrization of the probability of each of these edges how does an observed data point update these models? Algorithms that could efficiently perform updates of all models seems unlikely, but what constrictions could lead to efficient bayesian updates?
    \item Previous definitions of hypergraphs have noted a duality between nodes and edges, as in there is a one-to-one mapping between a graph with edge sets and vertex sets swapped in a well defined manner. How does this duality affect the spectral properties of the hypergraph connections?
    \item Neural Networks are constructed by creating a set of nodes, assigning each a number, and then imposing a directed graph structure with weighted edges from each layer to the next. This theory allows for a linear version of a neural network that connects multiple nodes in one layer to multiple nodes in the next while retaining single node connections.
    \item A common thread in modern algorithms is the ability to compute global properties of an object from local information. For example the PCP theorem states that a computational proof can be verified with high probability by only accessing a constant number of bits from the proof. Conceptually a global property of a string of bits can be verified through only local information. How can one define local and global properties on hypergraphs?
    \item  A current area of study known as Topological Data Analysis (TDA) attempts to study topological properties of an underlying manifold $X$ via a collection of 0 dimensional samples (aka points in $\mathbb{R}^n$). These points are then used to construct a graph via the usual Euclidean metric on
    \item Graphs induce a unique linear hypergraph via the action of powering. For example specifically look at edges in the space $V_2$.  
\end{itemize}

\section*{Acknowledgements}

how could this give a proof of compiler correctness?

what would a hypergraph binary tree look like?

\end{document}
